{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generative trajectory interpolation\n",
    "\n",
    "Wan, Z., & Dodge, S. (2023, November). A Generative Trajectory Interpolation Method for Imputing Gaps in Wildlife Movement Data. In *Proceedings of the 1st ACM SIGSPATIAL International Workshop on AI-driven Spatio-temporal Data Analysis for Wildlife Conservation* (pp. 1-8)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check available GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name())\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"GPU is unavailable\")\n",
    "    device = torch.device(\"cuda:1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Project and sample direction\n",
    "proj_dir = \"...\"\n",
    "sample_dir = \"...\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM-GAN-based trajectory interpolation model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EncoderLstm(nn.Module):\n",
    "    '''\n",
    "    LSTM path encoding module\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        super(EncoderLstm, self).__init__()\n",
    "        # The LSTM cell.\n",
    "        # Input dimension (observations mapped through embedding) is the same as the output\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "    def init_lstm(self, h, c):\n",
    "        # Initialization of the LSTM: hidden state and cell state\n",
    "        self.lstm_h = (h, c)\n",
    "\n",
    "    def forward(self, x, batch_size=None):\n",
    "        # batch size\n",
    "        if batch_size == None:\n",
    "            batch_size = x.shape[0]\n",
    "\n",
    "        # Reshape and applies LSTM over a whole sequence or over one single step\n",
    "        y, self.lstm_h = self.lstm(x, self.lstm_h)\n",
    "\n",
    "        return y, self.lstm_h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DecoderLstm(nn.Module):\n",
    "    '''\n",
    "    LSTM path decoding module\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, noise_len, n_latent_codes, slope=0.2):\n",
    "        super(DecoderLstm, self).__init__()\n",
    "        # Decoding LSTM\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.noise_len = noise_len\n",
    "        self.n_latent_codes = n_latent_codes\n",
    "\n",
    "        self.emd = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = torch.nn.LSTM(hidden_dim + noise_len + n_latent_codes, hidden_dim, num_layers=1, batch_first=True)\n",
    "        # Fully connected sub-network. Input is hidden_size, output is 2.\n",
    "        self.fc = nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(slope),\n",
    "                                torch.nn.Linear(hidden_dim, hidden_dim // 2), nn.LeakyReLU(slope),\n",
    "                                torch.nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "                                torch.nn.Linear(hidden_dim // 4, output_dim))\n",
    "\n",
    "        # init_weights(self)\n",
    "        self.lstm_h = []\n",
    "\n",
    "    def init_lstm(self, h, c):\n",
    "        # Initialization of the LSTM: hidden state and cell state\n",
    "        self.lstm_h = (h, c)\n",
    "\n",
    "    def forward(self, dec_inp, z, latent_code):\n",
    "        # batch size\n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        h = self.emd(dec_inp).view(batch_size, self.hidden_dim)\n",
    "        # For each sample in the batch, concatenate h (hidden state), z (noise), and latent_code\n",
    "        inp = torch.cat([h, z, latent_code], dim=1)\n",
    "        # Applies a forward step.\n",
    "        out, self.lstm_h = self.lstm(inp.unsqueeze(1), self.lstm_h)\n",
    "        # Applies the fully connected layer to the LSTM output\n",
    "        out = self.fc(out.squeeze())\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discriminator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LSTMDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_latent_code, bidirectional=False, slope=0.2):\n",
    "        super(LSTMDiscriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
    "        self.n_latent_code = n_latent_code\n",
    "\n",
    "        if bidirectional == True:\n",
    "            self.bd = 2\n",
    "        else:\n",
    "            self.bd = 1\n",
    "\n",
    "        self.hidden2label = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * self.bd, hidden_dim), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        # latent code inference: input is hidden_dim (concatenated encodings of observed and predicted trajectories), output is n_latent_code (distribution of latent codes)\n",
    "        self.latent_decoder = nn.Sequential(nn.Linear(hidden_dim * self.bd, hidden_dim // 2), nn.LeakyReLU(slope),\n",
    "                                            nn.Linear(hidden_dim // 2, n_latent_code))\n",
    "\n",
    "    def forward(self, x1, x2, y):\n",
    "        batch_size = x1.shape[0]\n",
    "        lstm_h_c0 = (torch.zeros(1, batch_size, self.hidden_dim).cuda(),\n",
    "                     torch.zeros(1, batch_size, self.hidden_dim).cuda())\n",
    "\n",
    "        # construct whole seqs\n",
    "        whole_seqs = torch.cat([x1, y, x2], dim=1)  # [batch_size, seq_len, input_dim]\n",
    "\n",
    "        # use lstm hidden states for classification and code inference\n",
    "        _, (hn, _) = self.lstm_encoder(whole_seqs, lstm_h_c0)\n",
    "        hidden = hn.view(batch_size, self.hidden_dim * self.bd)\n",
    "        label = self.hidden2label(hidden)\n",
    "        code_hat = self.latent_decoder(hidden)\n",
    "\n",
    "        return label, code_hat\n",
    "\n",
    "    def load(self, backup):\n",
    "        for m_from, m_to in zip(backup.modules(), self.modules()):\n",
    "            if isinstance(m_to, nn.Linear):\n",
    "                m_to.weight.data = m_from.weight.data.clone()\n",
    "                if m_to.bias is not None:\n",
    "                    m_to.bias.data = m_from.bias.data.clone()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LSTMGANIntp(nn.Module):\n",
    "    def __init__(\n",
    "            self, input_size=5, dec_output_dim=5, hidden_size=128, samp_int=300,\n",
    "            n_lstm_layers=1, noise_len=128,\n",
    "            y_max_len=100, disc_bd=False, n_latent_codes=2,\n",
    "            n_unrolling_steps=10, slope=0.2\n",
    "    ):\n",
    "        super(LSTMGANIntp, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.dec_output_dim = dec_output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_lstm_layers = n_lstm_layers\n",
    "\n",
    "        self.samp_int = samp_int\n",
    "        self.y_max_len = y_max_len\n",
    "        self.noise_len = noise_len\n",
    "        self.n_latent_codes = n_latent_codes\n",
    "        self.disc_bd = disc_bd\n",
    "        self.n_unrolling_steps = n_unrolling_steps\n",
    "\n",
    "        self.encoder1 = EncoderLstm(input_size, hidden_size, n_lstm_layers).cuda()\n",
    "        self.encoder2 = EncoderLstm(input_size, hidden_size, n_lstm_layers).cuda()\n",
    "        self.decoder = DecoderLstm(input_size, dec_output_dim, hidden_size, noise_len, n_latent_codes, slope).cuda()\n",
    "        self.D = LSTMDiscriminator(input_size, hidden_size, n_latent_codes, disc_bd, slope).cuda()\n",
    "\n",
    "        # concatenate x1 and x2 outputs\n",
    "        self.h_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size * 2), nn.LeakyReLU(slope),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        self.c_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size * 2), nn.LeakyReLU(slope),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "\n",
    "\n",
    "    def predict(self, x1, x2, noise, latent_code):\n",
    "        '''\n",
    "        x1: 1st part of the observed traj\n",
    "        x2: 2nd part of the observed traj\n",
    "        '''\n",
    "        batch_size = x1.shape[0]\n",
    "\n",
    "        # Initial values for the hidden and cell states (zero)\n",
    "        lstm_h_c = (torch.zeros(self.n_lstm_layers, batch_size, self.hidden_size).cuda(),\n",
    "                    torch.zeros(self.n_lstm_layers, batch_size, self.hidden_size).cuda())\n",
    "        self.encoder1.init_lstm(lstm_h_c[0], lstm_h_c[1])\n",
    "        self.encoder2.init_lstm(lstm_h_c[0], lstm_h_c[1])\n",
    "\n",
    "        # encode the observed sequence\n",
    "        _, x1_h = self.encoder1(x1, batch_size)\n",
    "        _, x2_h = self.encoder2(x2, batch_size)\n",
    "\n",
    "        # concatenation\n",
    "        enc_h_cat = (\n",
    "            self.h_fc(torch.cat([x1_h[0], x2_h[0]], dim=2)),\n",
    "            self.c_fc(torch.cat([x1_h[1], x2_h[1]], dim=2))\n",
    "        )\n",
    "\n",
    "        pred_ls = []\n",
    "\n",
    "        # last pt before gap\n",
    "        last_obsv = x1[:, -1, :]  # [batch_size, 5]\n",
    "        dec_inp = last_obsv\n",
    "\n",
    "        # init decoder hidden to the same as encoder\n",
    "        self.decoder.init_lstm(h=enc_h_cat[0], c=enc_h_cat[1])\n",
    "\n",
    "        # For all the steps to predict, applies a step of the decoder\n",
    "        for ii in range(self.y_max_len):\n",
    "            dec_out = self.decoder(dec_inp, noise, latent_code).view(batch_size, self.dec_output_dim)\n",
    "            # Keeps all the predictions\n",
    "            pred_ls.append(dec_out)\n",
    "            dec_inp = dec_out\n",
    "\n",
    "        pred = torch.stack(pred_ls, 1)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def train_model(\n",
    "            self, train_loader, epoch, train_batch_size, predictor_optimizer, D_optimizer,\n",
    "            scaler_relxy_vxy, criterion=nn.MSELoss(), loss_info_w=0.5, loss_dist_w=1\n",
    "    ):\n",
    "        # training\n",
    "        self.encoder1.train()\n",
    "        self.encoder2.train()\n",
    "        self.decoder.train()\n",
    "        self.D.train()\n",
    "\n",
    "        # loss\n",
    "        d_loss_batchsum = 0.0\n",
    "        g_loss_batchsum = 0.0\n",
    "        ADE_batchsum = 0.0\n",
    "        FDE_batchsum = 0.0\n",
    "        batch_skip = 0\n",
    "\n",
    "        for x1, x2, y, start_pts, x1_lens, x2_lens, y_lens in tqdm(train_loader):\n",
    "            if x1.shape[0] < train_batch_size:  # not enough data\n",
    "                batch_skip += 1  # skip the last batch\n",
    "                # print(f\"Batch skip. x1 shape: {x1.shape[0]}\")\n",
    "                continue\n",
    "\n",
    "                # data sent to gpu\n",
    "            x1 = x1.to(device)\n",
    "            x1_lens = x1_lens.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            y_lens = y_lens.to(device)\n",
    "\n",
    "            # zero the gradient for each optimizer\n",
    "            predictor_optimizer.zero_grad()\n",
    "            D_optimizer.zero_grad()\n",
    "\n",
    "            zeros = Variable(torch.zeros(train_batch_size, 1) + np.random.uniform(0, 0.1), requires_grad=False).cuda()\n",
    "            ones = Variable(torch.ones(train_batch_size, 1) * np.random.uniform(0.9, 1.0), requires_grad=False).cuda()\n",
    "            noise = torch.FloatTensor(torch.rand(train_batch_size, self.noise_len)).cuda()\n",
    "            latent_code = torch.FloatTensor(torch.rand(train_batch_size, self.n_latent_codes)).cuda()\n",
    "\n",
    "            # ============== Train Discriminator ================\n",
    "            for u in range(self.n_unrolling_steps + 1):\n",
    "                # Zero the gradient buffers of all parameters\n",
    "                self.D.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = self.predict(x1, x2, noise, latent_code)\n",
    "\n",
    "                fake_labels, code_hat = self.D(x1, x2, y_pred)  # classify fake samples\n",
    "                # Evaluate the MSE loss: the fake_labels should be close to zero\n",
    "                d_loss_fake = criterion(fake_labels, zeros)\n",
    "                d_loss_info = criterion(code_hat.view(train_batch_size, self.n_latent_codes), latent_code)\n",
    "                # Evaluate the MSE loss: the real should be close to one\n",
    "                real_labels, code_hat = self.D(x1, x2, y)  # classify real samples\n",
    "                d_loss_real = criterion(real_labels, ones)\n",
    "\n",
    "                #  loss functinos to use for D?\n",
    "                d_loss = d_loss_fake + d_loss_real\n",
    "                d_loss += loss_info_w * d_loss_info\n",
    "                d_loss.backward()  # update D\n",
    "                D_optimizer.step()\n",
    "\n",
    "                d_loss_batchsum += d_loss.item()  # loss.item() returns the loss as a float\n",
    "\n",
    "                if u == 0 and self.n_unrolling_steps > 0:\n",
    "                    backup = copy.deepcopy(self.D)\n",
    "\n",
    "            # =============== Train Generator =================\n",
    "            # Zero the gradient buffers of all the discriminator parameters\n",
    "            self.D.zero_grad()\n",
    "            # Zero the gradient buffers of all the generator parameters\n",
    "            predictor_optimizer.zero_grad()\n",
    "            # Applies a forward step of prediction\n",
    "            y_pred = self.predict(x1, x2, noise, latent_code)\n",
    "\n",
    "            # Classify the generated fake sample\n",
    "            gen_labels, code_hat = self.D(x1, x2, y_pred)\n",
    "            # distance loss between the predicted paths and the true ones\n",
    "            g_loss_dist = criterion(y_pred[:, :, :2], y[:, :, :2])\n",
    "            # attribute loss\n",
    "            g_loss_attr = criterion(y_pred[:, :, 2:], y[:, :, 2:])\n",
    "            # Adversarial loss (classification labels should be close to one)\n",
    "            g_loss_fooling = criterion(gen_labels, ones)\n",
    "            # Information loss\n",
    "            g_loss_info = criterion(code_hat.view(train_batch_size, self.n_latent_codes), latent_code)\n",
    "\n",
    "            # generator loss\n",
    "            g_loss = g_loss_fooling + loss_dist_w * g_loss_dist + g_loss_attr + loss_info_w * g_loss_info\n",
    "\n",
    "            g_loss.backward()\n",
    "            predictor_optimizer.step()\n",
    "\n",
    "            g_loss_batchsum += g_loss.item()\n",
    "\n",
    "            if self.n_unrolling_steps > 0:\n",
    "                self.D.load(backup)\n",
    "                del backup\n",
    "\n",
    "            # calculate error\n",
    "            with torch.no_grad():\n",
    "                ADE, FDE = calc_error(y_pred, y, scaler=scaler_relxy_vxy)\n",
    "\n",
    "            ADE_batchsum += ADE\n",
    "            FDE_batchsum += FDE\n",
    "\n",
    "        d_loss_batchavg = d_loss_batchsum / (len(train_loader) - batch_skip) / (self.n_unrolling_steps + 1)\n",
    "        g_loss_batchavg = g_loss_batchsum / (len(train_loader) - batch_skip)\n",
    "        ADE_batchavg = ADE_batchsum / (len(train_loader) - batch_skip)\n",
    "        FDE_batchavg = FDE_batchsum / (len(train_loader) - batch_skip)\n",
    "\n",
    "        # progress bar\n",
    "        print(f\"Epoch {epoch+1} | Training | d_loss {d_loss_batchavg:.4f} | g_loss {g_loss_batchavg:.4f}  \\\n",
    "              | ADE: {ADE_batchavg:.4f} | | FDE: {FDE_batchavg:.4f}\")\n",
    "\n",
    "        return d_loss_batchavg, g_loss_batchavg, ADE_batchavg, FDE_batchavg\n",
    "\n",
    "\n",
    "    def eval_model(\n",
    "            self, val_loader, epoch, val_batch_size, scaler_relxy_vxy, criterion=nn.MSELoss(),\n",
    "            loss_info_w=0.5, loss_dist_w=1\n",
    "    ):\n",
    "        # validating\n",
    "        self.encoder1.eval()\n",
    "        self.encoder2.eval()\n",
    "        self.decoder.eval()\n",
    "        self.D.eval()\n",
    "\n",
    "        # loss\n",
    "        d_loss_batchsum = 0.0\n",
    "        g_loss_batchsum = 0.0\n",
    "        ADE_batchsum = 0.0\n",
    "        FDE_batchsum = 0.0\n",
    "        batch_skip = 0\n",
    "\n",
    "        for x1, x2, y, start_pts, x1_lens, x2_lens, y_lens in tqdm(val_loader):\n",
    "            if x1.shape[0] < val_batch_size:  # not enough data\n",
    "                batch_skip += 1  # skip the last batch\n",
    "                # print(f\"Batch skip. x1 shape: {x1.shape[0]}\")\n",
    "                continue\n",
    "\n",
    "                # data sent to gpu\n",
    "            x1 = x1.to(device)\n",
    "            x1_lens = x1_lens.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            y_lens = y_lens.to(device)\n",
    "\n",
    "            zeros = Variable(torch.zeros(val_batch_size, 1) + np.random.uniform(0, 0.1), requires_grad=False).cuda()\n",
    "            ones = Variable(torch.ones(val_batch_size, 1) * np.random.uniform(0.9, 1.0), requires_grad=False).cuda()\n",
    "            noise = torch.FloatTensor(torch.rand(val_batch_size, self.noise_len)).cuda()\n",
    "            latent_code = torch.FloatTensor(torch.rand(val_batch_size, self.n_latent_codes)).cuda()\n",
    "\n",
    "            # ============== Validate Discriminator ================\n",
    "            # Zero the gradient buffers of all parameters\n",
    "            self.D.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.predict(x1, x2, noise, latent_code)\n",
    "\n",
    "                fake_labels, code_hat = self.D(x1, x2, y_pred)  # classify fake samples\n",
    "                # Evaluate the MSE loss: the fake_labels should be close to zero\n",
    "                d_loss_fake = criterion(fake_labels, zeros)\n",
    "                d_loss_info = criterion(code_hat.view(val_batch_size, self.n_latent_codes), latent_code)\n",
    "                # Evaluate the MSE loss: the real should be close to one\n",
    "                real_labels, code_hat = self.D(x1, x2, y)  # classify real samples\n",
    "                d_loss_real = criterion(real_labels, ones)\n",
    "\n",
    "                # descriminator loss\n",
    "                d_loss = d_loss_fake + d_loss_real\n",
    "                d_loss += loss_info_w * d_loss_info\n",
    "\n",
    "                d_loss_batchsum += d_loss.item()  # loss.item() returns the loss as a float\n",
    "\n",
    "\n",
    "            # =============== Validate Generator =================\n",
    "            # Zero the gradient buffers of all the discriminator parameters\n",
    "            self.D.zero_grad()\n",
    "            # Applies a forward step of prediction\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.predict(x1, x2, noise, latent_code)\n",
    "\n",
    "                # Classify the generated fake sample\n",
    "                gen_labels, code_hat = self.D(x1, x2, y_pred)\n",
    "                # L2 loss between the predicted paths and the true ones\n",
    "                g_loss_dist = criterion(y_pred[:, :, :2], y[:, :, :2])\n",
    "                # attribute loss\n",
    "                g_loss_attr = criterion(y_pred[:, :, 2:], y[:, :, 2:])\n",
    "                # Adversarial loss (classification labels should be close to one)\n",
    "                g_loss_fooling = criterion(gen_labels, ones)\n",
    "                # Information loss\n",
    "                g_loss_info = criterion(code_hat.view(val_batch_size, self.n_latent_codes), latent_code)\n",
    "\n",
    "                # generator loss\n",
    "                g_loss = g_loss_fooling + loss_dist_w * g_loss_dist + g_loss_attr + loss_info_w * g_loss_info\n",
    "\n",
    "                g_loss_batchsum += g_loss.item()\n",
    "\n",
    "\n",
    "            # calculate error\n",
    "            with torch.no_grad():\n",
    "                ADE, FDE = calc_error(y_pred, y, scaler=scaler_relxy_vxy)\n",
    "\n",
    "            ADE_batchsum += ADE\n",
    "            FDE_batchsum += FDE\n",
    "\n",
    "        d_loss_batchavg = d_loss_batchsum / (len(val_loader) - batch_skip)\n",
    "        g_loss_batchavg = g_loss_batchsum / (len(val_loader) - batch_skip)\n",
    "        ADE_batchavg = ADE_batchsum / (len(val_loader) - batch_skip)\n",
    "        FDE_batchavg = FDE_batchsum / (len(val_loader) - batch_skip)\n",
    "\n",
    "        # progress bar\n",
    "        print(f\"Validating | d_loss {d_loss_batchavg:.4f} | g_loss {g_loss_batchavg:.4f}  \\\n",
    "              | ADE: {ADE_batchavg:.4f} | | FDE: {FDE_batchavg:.4f}\")\n",
    "\n",
    "        return d_loss_batchavg, g_loss_batchavg, ADE_batchavg, FDE_batchavg\n",
    "\n",
    "\n",
    "    def test_model(self, test_loader, epoch, test_batch_size, scaler_relxy_vxy):\n",
    "        # validating\n",
    "        self.encoder1.eval()\n",
    "        self.encoder2.eval()\n",
    "        self.decoder.eval()\n",
    "        self.D.eval()\n",
    "\n",
    "        # loss\n",
    "        ADE_batchsum = 0.0\n",
    "        FDE_batchsum = 0.0\n",
    "        batch_skip = 0\n",
    "\n",
    "        for x1, x2, y, start_pts, x1_lens, x2_lens, y_lens in tqdm(test_loader):\n",
    "            if x1.shape[0] < test_batch_size:  # not enough data\n",
    "                batch_skip += 1  # skip the last batch\n",
    "                print(f\"Batch skip. x1 shape: {x1.shape[0]}\")\n",
    "                continue\n",
    "\n",
    "                # data sent to gpu\n",
    "            x1 = x1.to(device)\n",
    "            x1_lens = x1_lens.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            y_lens = y_lens.to(device)\n",
    "\n",
    "            noise = torch.FloatTensor(torch.rand(test_batch_size, self.noise_len)).cuda()\n",
    "            latent_code = torch.FloatTensor(torch.rand(test_batch_size, self.n_latent_codes)).cuda()\n",
    "\n",
    "            # =============== Validate Generator =================\n",
    "            # Zero the gradient buffers of all the discriminator parameters\n",
    "            self.D.zero_grad()\n",
    "            # Applies a forward step of prediction\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.predict(x1, x2, noise, latent_code)\n",
    "\n",
    "                # calculate error\n",
    "                ADE, FDE = calc_error(y_pred, y, scaler=scaler_relxy_vxy)\n",
    "                ADE_batchsum += ADE\n",
    "                FDE_batchsum += FDE\n",
    "\n",
    "        ADE_batchavg = ADE_batchsum / (len(test_loader) - batch_skip)\n",
    "        FDE_batchavg = FDE_batchsum / (len(test_loader) - batch_skip)\n",
    "\n",
    "        # progress bar\n",
    "        print(f\"Testing | Epoch {epoch+1} | ADE: {ADE_batchavg:.4f} | | FDE: {FDE_batchavg:.4f}\")\n",
    "\n",
    "        return ADE_batchavg, FDE_batchavg\n",
    "\n",
    "\n",
    "    def model_predict(self, x1, x2, latent_code=None, noise=None, y=None):\n",
    "        # predicting\n",
    "        self.encoder1.eval()\n",
    "        self.encoder2.eval()\n",
    "        self.decoder.eval()\n",
    "        self.D.eval()\n",
    "\n",
    "        # data sent to gpu\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "\n",
    "        pred_batch_size = x1.shape[0]\n",
    "        if noise is None:\n",
    "            noise = torch.FloatTensor(torch.rand(pred_batch_size, self.noise_len)).cuda()\n",
    "        if latent_code is None:\n",
    "            latent_code = torch.FloatTensor(torch.rand(pred_batch_size, self.n_latent_codes)).cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.predict(x1, x2, noise, latent_code)\n",
    "\n",
    "        if y is not None:\n",
    "            y = y.to(device)\n",
    "            ADE, FDE = calc_error(y_pred, y, scaler=scaler_relxy_vxy)\n",
    "            print(f\"ADE: {ADE}, FDE: {FDE}\")\n",
    "\n",
    "        return y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss/Error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_error(y_pred, y_gt, scaler=None):\n",
    "    '''\n",
    "    Average displacement error (ADE)\n",
    "    Final displacement error (FDE)\n",
    "\n",
    "    Input:\n",
    "        y_pred: torch.tensor\n",
    "            prediction\n",
    "        y_gt: torch.tensor\n",
    "        ground truth\n",
    "    '''\n",
    "    batch_size = y_pred.shape[0]\n",
    "    seq_len = y_pred.shape[1]\n",
    "    attr_dim = y_pred.shape[2]\n",
    "\n",
    "    # last pts\n",
    "    y_pred_last_pts = y_pred[:, -1, :].data.cpu().numpy()\n",
    "    y_gt_last_pts = y_gt[:, -1, :].data.cpu().numpy()\n",
    "\n",
    "    # convert 3d tensor to 2d array\n",
    "    y_pred = y_pred.view(batch_size * seq_len, attr_dim)  # 2d tensor\n",
    "    y_gt = y_gt.view(batch_size * seq_len, attr_dim)\n",
    "    y_pred = y_pred.data.cpu().numpy()\n",
    "    y_gt = y_gt.data.cpu().numpy()\n",
    "\n",
    "    if scaler is not None:\n",
    "        # inverse MinMaxScaler\n",
    "        y_gt_last_pts = scaler.inverse_transform(y_gt_last_pts)\n",
    "        y_pred_last_pts = scaler.inverse_transform(y_pred_last_pts)\n",
    "        y_pred = scaler.inverse_transform(y_pred)\n",
    "        y_gt = scaler.inverse_transform(y_gt)\n",
    "\n",
    "    fde = mean_squared_error(y_gt_last_pts[:, :2], y_pred_last_pts[:, :2], squared=False)\n",
    "    ade = mean_squared_error(y_gt[:, :2], y_pred[:, :2], squared=False)\n",
    "\n",
    "    return ade, fde"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(\n",
    "            self, model, train_loader, val_loader, scaler_relxy_vxy, save_dir, loss_dist_w, criterion=nn.MSELoss(),\n",
    "            n_epochs=200, patience=1, lr_g=1e-3, lr_d=1e-4, train_batch_size=32, val_batch_size=256\n",
    "    ):\n",
    "        self.scaler_relxy_vxy = scaler_relxy_vxy\n",
    "        self.loss_dist_w = loss_dist_w\n",
    "\n",
    "        self.n_epochs = n_epochs\n",
    "        self.patience = patience\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.predictor_optimizer = optim.Adam(\n",
    "            chain(\n",
    "                model.encoder1.parameters(), model.encoder2.parameters(), model.decoder.parameters(),\n",
    "                model.h_fc.parameters(), model.c_fc.parameters()\n",
    "            )\n",
    "            , lr=lr_g, betas=(0.9, 0.999)\n",
    "        )\n",
    "        self.D_optimizer = optim.Adam(model.D.parameters(), lr=lr_d, betas=(0.9, 0.999))\n",
    "        self.lr_g = lr_g\n",
    "        self.lr_d = lr_d\n",
    "\n",
    "\n",
    "    def fit(self, model, model_save_prefix, min_val_loss=None):\n",
    "        print(\"Training model...\")\n",
    "        model = model.to(device)\n",
    "\n",
    "        # initialize loss logger\n",
    "        if min_val_loss is None:\n",
    "            min_val_loss = float('inf')\n",
    "        else:\n",
    "            min_val_loss = min_val_loss\n",
    "        best_models = []\n",
    "        loss_log = []\n",
    "        model_save_name_log = []\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            train_d_loss, train_g_loss, train_ADE, train_FDE = model.train_model(\n",
    "                self.train_loader, epoch, self.train_batch_size, self.predictor_optimizer, self.D_optimizer,\n",
    "                self.scaler_relxy_vxy, criterion=nn.MSELoss(), loss_info_w=0.5, loss_dist_w=self.loss_dist_w\n",
    "            )\n",
    "            val_d_loss, val_g_loss, val_ADE, val_FDE = model.eval_model(\n",
    "                self.val_loader, epoch, self.val_batch_size, self.scaler_relxy_vxy, criterion=nn.MSELoss(),\n",
    "                loss_info_w=0.5, loss_dist_w=self.loss_dist_w\n",
    "            )\n",
    "\n",
    "            # save to log\n",
    "            loss_log.append(\n",
    "                [train_d_loss, train_g_loss, train_ADE, train_FDE,\n",
    "                 val_d_loss, val_g_loss, val_ADE, val_FDE]\n",
    "            )\n",
    "\n",
    "            # compare to min_val_loss\n",
    "            if val_ADE < min_val_loss:\n",
    "                min_val_loss = val_ADE\n",
    "                best_models.append([epoch, val_d_loss, val_g_loss, val_ADE, val_FDE])\n",
    "\n",
    "                if epoch > self.patience-1:\n",
    "                    model_save_name = model_save_prefix + f\"_epoch{epoch+1}_trainADE{train_ADE:.4f}_valADE{val_ADE:.4f}.pt\"\n",
    "\n",
    "                    # save model\n",
    "                    model_save_path = os.path.join(\n",
    "                        proj_dir, self.save_dir, model_save_name\n",
    "                    )\n",
    "                    torch.save(model, model_save_path)\n",
    "\n",
    "                    print(f\"val_ADE decreased to {val_ADE:.4f} at epoch {epoch+1}. Model saved.\")\n",
    "                    model_save_name_log.append(model_save_name)\n",
    "\n",
    "                else:\n",
    "                    print(f\"val_ADE decreased to {val_ADE:.4f} at epoch {epoch+1}. No save.\")\n",
    "                    model_save_name_log.append(\"no_save\")\n",
    "            else:\n",
    "                model_save_name_log.append(\"no_save\")\n",
    "\n",
    "        loss_log_arr = np.array(loss_log)\n",
    "        loss_log_df = pd.DataFrame(loss_log_arr, columns=[\n",
    "            \"train_d_loss\", \"train_g_loss\", \"train_ADE\", \"train_FDE\",\n",
    "            'val_d_loss', 'val_g_loss', 'val_ADE', 'val_FDE'\n",
    "        ])\n",
    "        loss_log_df[\"save_name\"] = model_save_name_log\n",
    "        self.loss_log_df = loss_log_df\n",
    "\n",
    "        return loss_log_df\n",
    "\n",
    "    def plot_train_log(self, loss_log_df=None):\n",
    "        if loss_log_df == None:\n",
    "            loss_log_df = self.loss_log_df\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "        x = np.array(range(len(self.loss_log_df)))\n",
    "\n",
    "        # loss\n",
    "        ax[0].plot(x, loss_log_df['train_g_loss'], color='dodgerblue', label='Generator loss')\n",
    "        ax[0].plot(x, loss_log_df['train_d_loss'], color='coral', label='Discriminator loss')\n",
    "        ax[0].plot(x, loss_log_df['val_g_loss'], color='blue', label='Generator loss')\n",
    "        ax[0].plot(x, loss_log_df['val_d_loss'], color='red', label='Discriminator loss')\n",
    "\n",
    "\n",
    "        # ade, fde\n",
    "        ax[1].plot(x, loss_log_df['train_ADE'], color='dodgerblue', label='ADE')\n",
    "        ax[1].plot(x, loss_log_df['train_FDE'], color='coral', label='FDE')\n",
    "        ax[1].plot(x, loss_log_df['val_ADE'], color='blue', label='ADE')\n",
    "        ax[1].plot(x, loss_log_df['val_FDE'], color='red', label='FDE')\n",
    "\n",
    "        ax[0].legend(loc=\"best\")\n",
    "        ax[1].legend(loc=\"best\")\n",
    "\n",
    "        ax[0].set_xlabel(\"Epoch\")\n",
    "        ax[0].set_ylabel(\"Loss\")\n",
    "        ax[1].set_xlabel(\"Epoch\")\n",
    "        ax[1].set_ylabel(\"Displacement error (m)\")\n",
    "\n",
    "        ax[0].set_title(\"Loss\")\n",
    "        ax[1].set_title(\"Displacement error\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset in pytorch format\n",
    "### Trajectory dataset class\n",
    "\n",
    "dtype has to be float (float32) instead of double (float64) in order to avoid data/parameter mismatch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tensorize_input(X1_ls, X2_ls, Y_gap_ls):\n",
    "    X1_ts_ls = []\n",
    "    X2_ts_ls = []\n",
    "    Y_ts_ls = []\n",
    "\n",
    "    # lengths\n",
    "    X1_len_ls = []\n",
    "    X2_len_ls = []\n",
    "    Y_len_ls = []\n",
    "\n",
    "    n_samples = len(X1_ls)\n",
    "\n",
    "    for spi in trange(n_samples):\n",
    "        X1_ts_ls.append(torch.tensor(X1_ls[spi], dtype=torch.float))\n",
    "        X2_ts_ls.append(torch.tensor(X2_ls[spi], dtype=torch.float))\n",
    "        Y_ts_ls.append(torch.tensor(Y_gap_ls[spi], dtype=torch.float))\n",
    "\n",
    "        X1_len_ls.append(len(X1_ls[spi]))\n",
    "        X2_len_ls.append(len(X2_ls[spi]))\n",
    "        Y_len_ls.append(len(Y_gap_ls[spi]))\n",
    "\n",
    "    return X1_ts_ls, X2_ts_ls, Y_ts_ls, X1_len_ls, X2_len_ls, Y_len_ls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class IntpTrajDataset(Dataset):\n",
    "    def __init__(self, X1_ls, X2_ls, Y_gap_ls, start_xy_arr):\n",
    "        X1_ts_ls, X2_ts_ls, Y_ts_ls, X1_len_ls, X2_len_ls, Y_len_ls = tensorize_input(X1_ls, X2_ls, Y_gap_ls)\n",
    "\n",
    "        # max len\n",
    "        x_max_len = 0\n",
    "        y_max_len = 0\n",
    "        for sp_i in range(len(X1_ts_ls)):\n",
    "            # x_max_len\n",
    "            if len(X1_ts_ls[sp_i]) > x_max_len:\n",
    "                x_max_len = len(X1_ts_ls[sp_i])\n",
    "            if len(X2_ts_ls[sp_i]) > x_max_len:\n",
    "                x_max_len = len(X2_ts_ls[sp_i])\n",
    "            # y_max_len\n",
    "            if len(Y_ts_ls[sp_i]) > y_max_len:\n",
    "                y_max_len = len(Y_ts_ls[sp_i])\n",
    "\n",
    "        # pad first seq to desired length\n",
    "        X1_ts_ls[0] = nn.ConstantPad2d((0, 0, 0, x_max_len - X1_ts_ls[0].shape[0]), 0)(X1_ts_ls[0])\n",
    "        X2_ts_ls[0] = nn.ConstantPad2d((0, 0, 0, x_max_len - X2_ts_ls[0].shape[0]), 0)(X2_ts_ls[0])\n",
    "        Y_ts_ls[0] = nn.ConstantPad2d((0, 0, 0, y_max_len - Y_ts_ls[0].shape[0]), 0)(Y_ts_ls[0])\n",
    "\n",
    "        # pad all seqs to desired length\n",
    "        self.X1 = pad_sequence(X1_ts_ls, batch_first=True)\n",
    "        self.X2 = pad_sequence(X2_ts_ls, batch_first=True)\n",
    "        self.Y = pad_sequence(Y_ts_ls, batch_first=True)\n",
    "        print(f\"padded shape: X1: {self.X1.shape}, X2: {self.X2.shape}, Y: {self.Y.shape}\")\n",
    "\n",
    "        # start_xy_arr: np.object -> float\n",
    "        self.start_pts = torch.tensor(start_xy_arr.astype(float), dtype=torch.float)\n",
    "\n",
    "        # lengths\n",
    "        self.X1_lens = torch.tensor(X1_len_ls, dtype=torch.int)\n",
    "        self.X2_lens = torch.tensor(X2_len_ls, dtype=torch.int)\n",
    "        self.Y_lens = torch.tensor(Y_len_ls, dtype=torch.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X1.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X1[i,:,:], self.X2[i,:,:], self.Y[i,:,:], self.start_pts[i, :], self.X1_lens[i], self.X2_lens[i], self.Y_lens[i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training, validation, and test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_val_test_set(X1_ls, X2_ls, Y_gap_ls, start_xy_arr, train_pct=0.7):\n",
    "    n_samples = len(X1_ls)\n",
    "\n",
    "    # train, val, test split\n",
    "    val_test_pct = 1 - train_pct\n",
    "    test_pct = 0.33  # test/(val+test)\n",
    "    X1_train, X1_test, X2_train, X2_test, Y_train, Y_test, start_xy_arr_train, start_xy_arr_test = train_test_split(\n",
    "        X1_ls, X2_ls, Y_gap_ls, start_xy_arr, test_size=val_test_pct\n",
    "    )\n",
    "    X1_val, X1_test, X2_val, X2_test, Y_val, Y_test, start_xy_arr_val, start_xy_arr_test = train_test_split(\n",
    "        X1_test, X2_test, Y_test, start_xy_arr_test, test_size=test_pct\n",
    "    )\n",
    "\n",
    "    print(\"Train, validation, test split done! Next: create PyTorch datasets\")\n",
    "    # PyTorch dataset\n",
    "    train_set = IntpTrajDataset(X1_train, X2_train, Y_train, start_xy_arr_train)\n",
    "    val_set = IntpTrajDataset(X1_val, X2_val, Y_val, start_xy_arr_val)\n",
    "    test_set = IntpTrajDataset(X1_test, X2_test, Y_test, start_xy_arr_test)\n",
    "\n",
    "    return train_set, val_set, test_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read pickle\n",
    "pk_name = \"...stork.pickle\"  # preprocessed strok tracking dataset in the pickle format\n",
    "with open(os.path.join(proj_dir, pk_name), 'rb') as my_file_obj:\n",
    "    pts_df, scaler_relxy_vxy = pickle.load(my_file_obj)\n",
    "\n",
    "print(f\"Number of tracking points: {len(pts_df)}\")\n",
    "pts_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "\n",
    "y_max_len = seq_len // 2\n",
    "if y_max_len % 2 != 0:\n",
    "    y_max_len += 1\n",
    "\n",
    "# read pickle\n",
    "pk_name = f\"...{seq_len}.pickle\"  # samples containing lists of X1, X2, Y, and gap start pt\n",
    "with open(os.path.join(sample_dir, pk_name), 'rb') as my_file_obj:\n",
    "    X1_ls, X2_ls, Y_gap_ls, start_xy_arr = pickle.load(my_file_obj)\n",
    "\n",
    "print(f\"Number of samples: {len(X1_ls)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create training datasets\n",
    "train_set, val_set, test_set = create_toy_dataset(\n",
    "    X1_ls, X2_ls, Y_gap_ls, start_xy_arr, size=20000, train_pct=0.7\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Val set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# save to pickle\n",
    "pk_name = f\"....pickle\"\n",
    "with open(os.path.join(proj_dir, \"...\", pk_name), 'wb') as my_file_obj:\n",
    "    pickle.dump([train_set, val_set, test_set], my_file_obj)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read pickle\n",
    "pk_name = f\"....pickle\"\n",
    "with open(os.path.join(sample_dir, pk_name), 'rb') as my_file_obj:\n",
    "    train_set, val_set, test_set = pickle.load(my_file_obj)\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Val set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training datasets\n",
    "train_batch_size = 32\n",
    "val_batch_size = 64\n",
    "test_batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=val_batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=test_batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training example: with distance loss weight 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_max_len = seq_len // 2\n",
    "if y_max_len % 2 != 0:\n",
    "    y_max_len += 1\n",
    "\n",
    "gan_intp_model = LSTMGANIntp(\n",
    "    input_size=5, dec_output_dim=5, hidden_size=128, n_lstm_layers=1, noise_len=32,\n",
    "    y_max_len=y_max_len, disc_bd=False, n_latent_codes=2, n_unrolling_steps=10, slope=0.2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    gan_intp_model, train_loader, val_loader, scaler_relxy_vxy, save_dir=f\"...\", loss_dist_w=1,\n",
    "    criterion=nn.MSELoss(), n_epochs=100, patience=3, lr_g=1e-3, lr_d=1e-4, train_batch_size=train_batch_size, val_batch_size=val_batch_size\n",
    ")\n",
    "\n",
    "model_save_prefix = f\"...\"\n",
    "loss_log_df = trainer.fit(gan_intp_model, model_save_prefix)\n",
    "\n",
    "# save to pickle\n",
    "pk_name = f\"....pickle\"\n",
    "with open(os.path.join(proj_dir, f\"...\", pk_name), 'wb') as my_file_obj:\n",
    "    pickle.dump(loss_log_df, my_file_obj)\n",
    "\n",
    "# print the best model info\n",
    "ade_min_idx = loss_log_df['val_ADE'].idxmin()\n",
    "print(f\"Min validation ADE {loss_log_df.loc[ade_min_idx, 'val_ADE']} at epoch {ade_min_idx+1}\")\n",
    "print(f\"with training ADE {loss_log_df.loc[ade_min_idx, 'train_ADE']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
